## Compte Rendu ‚Äì Projet de Mod√©lisation

## Par : EL YESRI Inass

## 1Ô∏è‚É£ Introduction

## üîπ Contexte

Ce projet a pour objectif de construire un mod√®le de pr√©diction √† partir d‚Äôun dataset r√©el. Les donn√©es initiales sont brutes : elles peuvent contenir des valeurs manquantes, des incoh√©rences, des variables cat√©gorielles, des outliers, etc.

L‚Äôenjeu est de transformer ces donn√©es en une base exploitable pour entra√Æner un mod√®le de Machine Learning fiable.

## üîπ Probl√©matique

## La probl√©matique principale est la suivante :

Comment obtenir un mod√®le performant et interpr√©table √† partir de donn√©es imparfaites (bruit, valeurs manquantes, potentiellement d√©s√©quilibr√©es) ?

## üîπ Objectifs

Nettoyer et pr√©parer le dataset.

Choisir et entra√Æner un ou plusieurs algorithmes de Machine Learning.

√âvaluer les mod√®les via : Accuracy, F1-Score, RMSE, ROC-AUC.

Analyser les erreurs avec la matrice de confusion.

Discuter les limites et proposer des pistes d‚Äôam√©lioration.

## 2Ô∏è‚É£ M√©thodologie

## 2.1. Chargement & exploration des donn√©es

## import pandas as pd

## # Charger le dataset

df = pd.read_csv("data.csv")  # üëâ √† adapter avec le vrai chemin

## # Aper√ßu des premi√®res lignes

## print(df.head())

## # Infos g√©n√©rales

## print(df.info())

## # Statistiques descriptives

## print(df.describe())

## üîç Lecture

head() permet de voir les premi√®res lignes et comprendre la structure.

info() montre les types de variables et les valeurs manquantes.

describe() donne des stats de base (moyenne, min, max, etc.) utiles pour rep√©rer les anomalies.

## 2.2. Gestion des valeurs manquantes

Exemple : imputation simple (num√©riques ‚Üí moyenne, cat√©gorielles ‚Üí mode).

## from sklearn.impute import SimpleImputer

## import numpy as np

## # S√©parer features et target

X = df.drop(columns=["target"])  # üëâ adapter le nom de la cible

## y = df["target"]

## # S√©parer variables num√©riques / cat√©gorielles

num_cols = X.select_dtypes(include=np.number).columns

cat_cols = X.select_dtypes(exclude=np.number).columns

## num_imputer = SimpleImputer(strategy="mean")

cat_imputer = SimpleImputer(strategy="most_frequent")

X[num_cols] = num_imputer.fit_transform(X[num_cols])

## if len(cat_cols) > 0:

    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])

## üßæ Lecture

On √©vite de supprimer trop de lignes : on pr√©f√®re imputer.

Moyenne pour les num√©riques : √©vite de cr√©er des valeurs extr√™mes.

Mode pour les cat√©gorielles : permet de garder une modalit√© existante.

## 2.3. Encodage & Normalisation

On encode les variables cat√©gorielles et on met les variables √† la m√™me √©chelle (utile pour SVM, r√©gression logistique‚Ä¶).

from sklearn.preprocessing import OneHotEncoder, StandardScaler

## from sklearn.compose import ColumnTransformer

from sklearn.model_selection import train_test_split

## from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier

## # Encodage + scaling dans un pipeline

## preprocessor = ColumnTransformer(

## transformers=[

## ("num", StandardScaler(), num_cols),

        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)

## ]

## )

# Exemple avec un RandomForest (tu peux changer de mod√®le)

## model = RandomForestClassifier(

## n_estimators=200,

## random_state=42

## )

## pipeline = Pipeline(

## steps=[

## ("preprocess", preprocessor),

## ("model", model)

## ]

## )

## # Split Train/Test

X_train, X_test, y_train, y_test = train_test_split(

## X, y, test_size=0.2, random_state=42, stratify=y

## )

## üßæ Lecture

Le ColumnTransformer applique diff√©rents traitements selon le type de variable.

Le Pipeline garantit que le m√™me pr√©traitement est appliqu√© √† train et test, ce qui √©vite les fuites de donn√©es.

stratify=y garde le m√™me √©quilibre entre classes dans train et test.

## 2.4. Entra√Ænement du mod√®le

## # Entra√Ænement

## pipeline.fit(X_train, y_train)

## 3Ô∏è‚É£ R√©sultats & Discussion

3.1. Calcul des m√©triques (Accuracy, F1, ROC-AUC, RMSE)

## from sklearn.metrics import (

## accuracy_score, f1_score,

## roc_auc_score, mean_squared_error

## )

## import numpy as np

## # Pr√©dictions classes

## y_pred = pipeline.predict(X_test)

# Si le mod√®le supporte predict_proba (RF, LR, etc.)

if hasattr(pipeline.named_steps["model"], "predict_proba"):

    y_proba = pipeline.predict_proba(X_test)[:, 1]  # binaire

## else:

## y_proba = None

## accuracy = accuracy_score(y_test, y_pred)

f1 = f1_score(y_test, y_pred, average="weighted")  # "binary" ou "macro" selon le cas

# RMSE sur les classes pr√©dictes (moins courant, mais possible)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# ROC-AUC (binaire). Si multi-classe ‚Üí One-vs-Rest (√† adapter).

## if y_proba is not None and len(y.unique()) == 2:

## roc_auc = roc_auc_score(y_test, y_proba)

## else:

## roc_auc = None

## print(f"Accuracy  : {accuracy:.4f}")

## print(f"F1-score  : {f1:.4f}")

## print(f"RMSE      : {rmse:.4f}")

## if roc_auc is not None:

## print(f"ROC-AUC   : {roc_auc:.4f}")

## üßæ Lecture des m√©triques

Accuracy : proportion globale de bonnes pr√©dictions.

F1-score : √©quilibre entre pr√©cision et rappel, utile si les classes sont d√©s√©quilibr√©es.

RMSE : racine de l‚Äôerreur quadratique moyenne ‚Üí plus il est faible, mieux c‚Äôest (souvent utilis√© pour la r√©gression, ici appliqu√© sur les classes).

ROC-AUC : capacit√© du mod√®le √† s√©parer les classes positives et n√©gatives (0.5 = hasard, 1.0 = parfait).

## 3.2. Matrice de confusion + graphique

## from sklearn.metrics import confusion_matrix

## import matplotlib.pyplot as plt

## import seaborn as sns

## cm = confusion_matrix(y_test, y_pred)

## plt.figure(figsize=(6, 5))

## sns.heatmap(

## cm,

## annot=True,

## fmt="d",

## cmap="Blues",

## xticklabels=sorted(y.unique()),

## yticklabels=sorted(y.unique())

## )

## plt.xlabel("Pr√©dictions")

## plt.ylabel("V√©rit√©s terrain")

## plt.title("Matrice de confusion")

## plt.tight_layout()

## plt.show()

## üìä Lecture du graphique ‚Äì Matrice de confusion

La diagonale repr√©sente les bonnes pr√©dictions.

Les valeurs hors diagonale sont les erreurs de classification.

Si une classe est souvent pr√©dite comme une autre, cela r√©v√®le :

soit un probl√®me de donn√©es (variables pas assez discriminantes),

soit un d√©s√©quilibre ‚Üí le mod√®le ‚Äú√©crase‚Äù les classes minoritaires.

## 3.3. Courbe ROC (si probl√®me binaire)

from sklearn.metrics import roc_curve, roc_auc_score

## if y_proba is not None and len(y.unique()) == 2:

## fpr, tpr, thresholds = roc_curve(y_test, y_proba)

## roc_auc = roc_auc_score(y_test, y_proba)

## plt.figure(figsize=(6, 5))

    plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")

    plt.plot([0, 1], [0, 1], linestyle="--")  # ligne hasard

## plt.xlabel("Taux de faux positifs (FPR)")

## plt.ylabel("Taux de vrais positifs (TPR)")

## plt.title("Courbe ROC")

## plt.legend(loc="lower right")

## plt.tight_layout()

## plt.show()

## üìä Lecture du graphique ‚Äì Courbe ROC

Plus la courbe est au-dessus de la diagonale, plus le mod√®le s√©pare bien les classes.

Un AUC proche de 1.0 = excellent, proche de 0.5 = mod√®le inutile (√©quivalent au hasard).

Permet de comparer plusieurs mod√®les ind√©pendamment du seuil de d√©cision.

3.4. Importance des variables (Feature Importance ‚Äì mod√®le d‚Äôarbres)

## import numpy as np

## model = pipeline.named_steps["model"]

# Attention : pour avoir les bons noms de colonnes apr√®s OneHotEncoder,

## # il faut les r√©cup√©rer depuis le preprocessor :

## feature_names = []

# Noms des variables num√©riques (apr√®s scaling, m√™me nom)

## feature_names.extend(num_cols)

# Noms des variables cat√©gorielles apr√®s OneHotEncoder

## if len(cat_cols) > 0:

    ohe = pipeline.named_steps["preprocess"].named_transformers_["cat"]

    ohe_features = list(ohe.get_feature_names_out(cat_cols))

## feature_names.extend(ohe_features)

## importances = model.feature_importances_

## indices = np.argsort(importances)[::-1]

## top_k = 10  # afficher les 10 plus importantes

top_features = [feature_names[i] for i in indices[:top_k]]

## top_importances = importances[indices[:top_k]]

## plt.figure(figsize=(8, 5))

plt.barh(top_features[::-1], top_importances[::-1])

## plt.xlabel("Importance")

plt.title("Top 10 des features les plus importantes")

## plt.tight_layout()

## plt.show()

üìä Lecture du graphique ‚Äì Importance des variables

Permet d‚Äôidentifier les variables qui influencent le plus les pr√©dictions.

Si une variable jug√©e importante par le domaine m√©tier appara√Æt tr√®s faible ici, cela peut signaler :

## un d√©faut de preprocessing,

une mauvaise qualit√© de la donn√©e pour cette feature,

ou la n√©cessit√© de revoir le mod√®le.

## 4Ô∏è‚É£ Conclusion

## üîπ Limites du mod√®le

D√©pendance forte √† la qualit√© des donn√©es (bruit, valeurs manquantes, erreurs de saisie).

√âventuel d√©s√©quilibre entre classes qui p√©nalise le F1-score et la d√©tection de classes minoritaires.

Mod√®le possiblement sensible aux hyperparam√®tres (notamment pour SVM, XGBoost, etc.).

RMSE reste une m√©trique moins intuitive pour la classification.

## üîπ Pistes d'am√©lioration

Am√©liorer le nettoyage et l'enrichissement des donn√©es.

Utiliser une recherche syst√©matique d‚Äôhyperparam√®tres (GridSearchCV, RandomizedSearchCV).

Tester des mod√®les plus avanc√©s : XGBoost, LightGBM, CatBoost.

G√©rer explicitement le d√©s√©quilibre des classes (SMOTE, class_weight).

Ajouter des m√©thodes d‚Äôexplainability (SHAP, LIME) pour analyser finement les d√©cisions du mod√®le.